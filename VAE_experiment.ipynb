{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logcosh VAE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0WRWJJj84Iz"
      },
      "source": [
        "Копируем репозиторий с моделями."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tIiidD282GQ"
      },
      "source": [
        "!git clone https://github.com/AntixK/PyTorch-VAE\n",
        "import os\n",
        "os.chdir(\"/content/PyTorch-VAE\")\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe23W2PX9Nkq"
      },
      "source": [
        "Загружаем папки с фотографием."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iezxyHPJ2G4n",
        "outputId": "ca5c9ee4-8aea-4df7-fcec-68305682545d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWczfTWk9hyK"
      },
      "source": [
        "Код для создания набора данных CelebA, в случае неработания функции библитечной."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW9BSlLNdJaU"
      },
      "source": [
        "import os\n",
        "import zipfile \n",
        "import gdown\n",
        "import torch\n",
        "from natsort import natsorted\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "\n",
        "import torchvision as vision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.manifold import TSNE\n",
        "# from torchvision import transform\n",
        "\n",
        "## Setup\n",
        "# Number of gpus available\n",
        "ngpu = 1\n",
        "device = torch.device('cuda:0' if (\n",
        "    torch.cuda.is_available() and ngpu > 0) else 'cpu')\n",
        "\n",
        "## Fetch data from Google Drive \n",
        "# Root directory for the dataset\n",
        "data_root = './my_data'\n",
        "# Path to folder with the dataset\n",
        "dataset_folder = f'{data_root}/img_align_celeba'\n",
        "# URL for the CelebA dataset\n",
        "url = 'https://drive.google.com/uc?id=1cNIac61PSA_LqDFYFUeyaQYekYPc75NH'\n",
        "# Path to download the dataset to\n",
        "download_path = f'{data_root}/img_align_celeba.zip'\n",
        "\n",
        "# Create required directories \n",
        "if not os.path.exists(data_root):\n",
        "  os.makedirs(data_root)\n",
        "  os.makedirs(dataset_folder)\n",
        "\n",
        "gdown.download(url, download_path, quiet=False)\n",
        "\n",
        "# Unzip the downloaded file \n",
        "with zipfile.ZipFile(download_path, 'r') as ziphandler:\n",
        "  ziphandler.extractall(dataset_folder)\n",
        "\n",
        "## Create a custom Dataset class\n",
        "class CelebADataset(Dataset):\n",
        "  def __init__(self, root_dir, transform=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      root_dir (string): Directory with all the images\n",
        "      transform (callable, optional): transform to be applied to each image sample\n",
        "    \"\"\"\n",
        "    # Read names of images in the root directory\n",
        "    image_names = os.listdir(root_dir)\n",
        "\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform \n",
        "    self.image_names = natsorted(image_names)\n",
        "\n",
        "  def __len__(self): \n",
        "    return len(self.image_names)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Get the path to the image \n",
        "    img_path = os.path.join(self.root_dir, self.image_names[idx])\n",
        "    # Load image and convert it to RGB\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    # Apply transformations to the image\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "## Load the dataset \n",
        "# Path to directory with all the images\n",
        "img_folder = f'{dataset_folder}/img_align_celeba'\n",
        "# Spatial size of training images, images are resized to this size.\n",
        "image_size = 64\n",
        "# Transformations to be applied to each individual image sample\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                          std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "# Load the dataset from file and apply transformations\n",
        "celeba_dataset = CelebADataset(img_folder, transform)\n",
        "\n",
        "## Create a dataloader \n",
        "# Batch size during training\n",
        "batch_size = 64\n",
        "# Number of workers for the dataloader\n",
        "num_workers = 0 if device.type == 'cuda' else 2\n",
        "# Whether to put fetched data tensors to pinned memory\n",
        "pin_memory = True if device.type == 'cuda' else False\n",
        "\n",
        "celeba_dataloader = torch.utils.data.DataLoader(celeba_dataset,\n",
        "                                                batch_size=batch_size,\n",
        "                                                num_workers=num_workers,\n",
        "                                                pin_memory=pin_memory,\n",
        "                                                shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCZEzczVCW6W"
      },
      "source": [
        "Основная модель, в которой можно менять архитектуру и функцию потерь"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhY7a0TIgm6w"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "os.chdir(\"/content/PyTorch-VAE\")\n",
        "from models.types_ import *\n",
        "from scipy.spatial import distance_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from models import BaseVAE\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class LogCoshVAE(BaseVAE):\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 latent_dim: int,\n",
        "                 hidden_dims: List = None,\n",
        "                 alpha: float = 100.,\n",
        "                 beta: float = 10.,\n",
        "                 gamma1: float = 0.,\n",
        "                 gamma2: float = 0.,\n",
        "                 teta1: float = 0.,\n",
        "                 teta2: float = 0.,\n",
        "                 **kwargs) -> None:\n",
        "        super(LogCoshVAE, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma1 = gamma1\n",
        "        self.gamma2 = gamma2\n",
        "        self.teta1 = teta1\n",
        "        self.teta2 = teta2\n",
        "        modules = []\n",
        "        if hidden_dims is None:\n",
        "            hidden_dims = [32, 64, 128, 256, 512]\n",
        "\n",
        "        # Build Encoder\n",
        "        for h_dim in hidden_dims:\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
        "                              kernel_size= 3, stride= 2, padding  = 1),\n",
        "                    nn.BatchNorm2d(h_dim),\n",
        "                    nn.LeakyReLU())\n",
        "            )\n",
        "            in_channels = h_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(*modules)\n",
        "        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
        "        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
        "\n",
        "\n",
        "        # Build Decoder\n",
        "        modules = []\n",
        "\n",
        "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
        "\n",
        "        hidden_dims.reverse()\n",
        "\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ConvTranspose2d(hidden_dims[i],\n",
        "                                       hidden_dims[i + 1],\n",
        "                                       kernel_size=3,\n",
        "                                       stride = 2,\n",
        "                                       padding=1,\n",
        "                                       output_padding=1),\n",
        "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
        "                    nn.LeakyReLU())\n",
        "            )\n",
        "\n",
        "        self.decoder = nn.Sequential(*modules)\n",
        "\n",
        "        self.final_layer = nn.Sequential(\n",
        "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
        "                                               hidden_dims[-1],\n",
        "                                               kernel_size=3,\n",
        "                                               stride=2,\n",
        "                                               padding=1,\n",
        "                                               output_padding=1),\n",
        "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
        "                            nn.LeakyReLU(),\n",
        "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
        "                                      kernel_size= 3, padding= 1),\n",
        "                            nn.Tanh())\n",
        "\n",
        "    def encode(self, input: Tensor) -> List[Tensor]:\n",
        "        \"\"\"\n",
        "        Encodes the input by passing through the encoder network\n",
        "        and returns the latent codes.\n",
        "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
        "        :return: (Tensor) List of latent codes\n",
        "        \"\"\"\n",
        "        result = self.encoder(input)\n",
        "        result = torch.flatten(result, start_dim=1)\n",
        "\n",
        "        # Split the result into mu and var components\n",
        "        # of the latent Gaussian distribution\n",
        "        mu = self.fc_mu(result)\n",
        "        log_var = self.fc_var(result)\n",
        "\n",
        "        return [mu, log_var]\n",
        "\n",
        "    def decode(self, z: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Maps the given latent codes\n",
        "        onto the image space.\n",
        "        :param z: (Tensor) [B x D]\n",
        "        :return: (Tensor) [B x C x H x W]\n",
        "        \"\"\"\n",
        "        result = self.decoder_input(z)\n",
        "        result = result.view(-1, 512, 2, 2)\n",
        "        result = self.decoder(result)\n",
        "        result = self.final_layer(result)\n",
        "        return result\n",
        "\n",
        "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Reparameterization trick to sample from N(mu, var) from\n",
        "        N(0,1).\n",
        "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
        "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
        "        :return: (Tensor) [B x D]\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps * std + mu\n",
        "\n",
        "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
        "        mu, log_var = self.encode(input)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return  [self.decode(z), input, mu, log_var]\n",
        "\n",
        "    def loss_function(self,\n",
        "                      *args,\n",
        "                      **kwargs) -> dict:\n",
        "        \"\"\"\n",
        "        Computes the VAE loss function.\n",
        "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
        "        :param args:\n",
        "        :param kwargs:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        recons = args[0]\n",
        "        input = args[1]\n",
        "        mu = args[2]\n",
        "        log_var = args[3]\n",
        "\n",
        "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
        "        target = kwargs['target']\n",
        "        t = recons - input\n",
        "        # recons_loss = F.mse_loss(recons, input)\n",
        "        # cosh = torch.cosh(self.alpha * t)\n",
        "        # recons_loss = (1./self.alpha * torch.log(cosh)).mean()\n",
        "\n",
        "        recons_loss = self.alpha * t + \\\n",
        "                      torch.log(1. + torch.exp(- 2 * self.alpha * t)) - \\\n",
        "                      torch.log(torch.tensor(2.0))\n",
        "        # print(self.alpha* t.max(), self.alpha*t.min())\n",
        "        recons_loss = (1. / self.alpha) * recons_loss.mean()\n",
        "\n",
        "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
        "\n",
        "        ones = torch.ones(48) #.to(\"cuda\")                                                        !!!!! добавить cuda в случае обучения на видеокарте\n",
        "        zeros = torch.zeros(80) #.to(\"cuda\")                                                      !!!!! добавить cuda в случае обучения на видеокарте\n",
        "        ones_zeros = torch.cat([ones,zeros])\n",
        "\n",
        "        new_mu = F.normalize(mu * ones_zeros, p=2, dim=1)\n",
        "        new_log_var = F.normalize(log_var * ones_zeros, p=2, dim=1)\n",
        "\n",
        "        my_loss_mu = torch.cdist(new_mu, new_mu, 2)       \n",
        "        my_loss_log_var = torch.cdist(new_log_var, new_log_var, 2)\n",
        "        new_target = target.unsqueeze(1).type(torch.FloatTensor) # !!!!!!!!!! добавить cuda в случае обучения на видеокарте\n",
        "        temp = torch.cdist(new_target, new_target, 1)\n",
        "        not_sim = (temp != 0).type(torch.FloatTensor) # !!!!!!!!!! добавить cuda в случае обучения на видеокарте\n",
        "        sim = (temp == 0).type(torch.FloatTensor) # !!!!!!!!! добавить cuda в случае обучения на видеокарте\n",
        "\n",
        "        loss = recons_loss + self.beta * kld_weight * kld_loss + (self.gamma1 * torch.sum(my_loss_mu * sim) / 2) - (self.gamma2 * torch.sum(my_loss_mu * not_sim) / 2) + (self.teta1 * torch.sum(my_loss_log_var * sim) / 2) - (self.teta2 * torch.sum(my_loss_log_var * not_sim) / 2)\n",
        "        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':-kld_loss}\n",
        "\n",
        "    def sample(self,\n",
        "               num_samples:int,\n",
        "               current_device: int, **kwargs) -> Tensor:\n",
        "        \"\"\"\n",
        "        Samples from the latent space and return the corresponding\n",
        "        image space map.\n",
        "        :param num_samples: (Int) Number of samples\n",
        "        :param current_device: (Int) Device to run the model\n",
        "        :return: (Tensor)\n",
        "        \"\"\"\n",
        "        z = torch.randn(num_samples,\n",
        "                        self.latent_dim)\n",
        "\n",
        "        z = z.to(current_device)\n",
        "\n",
        "        samples = self.decode(z)\n",
        "        return samples\n",
        "\n",
        "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
        "        \"\"\"\n",
        "        Given an input image x, returns the reconstructed image\n",
        "        :param x: (Tensor) [B x C x H x W]\n",
        "        :return: (Tensor) [B x C x H x W]\n",
        "        \"\"\"\n",
        "\n",
        "        return self.forward(x)[0]\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BWGfX2zCuh8"
      },
      "source": [
        "Обёртка над моделью для генерации датасетов и других полезных вещей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEzqSox8VVdU"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "os.chdir(\"/content/PyTorch-VAE\")\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch import optim\n",
        "from models import BaseVAE\n",
        "from models.types_ import *\n",
        "from utils import data_loader\n",
        "import pytorch_lightning as pl\n",
        "from torchvision import transforms\n",
        "import torchvision.utils as vutils\n",
        "from torchvision.datasets import CelebA\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "\n",
        "\n",
        "import gdown\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from natsort import natsorted\n",
        "from PIL import Image\n",
        "from copy import deepcopy\n",
        "\n",
        "from pytorch_lightning.logging import TestTubeLogger\n",
        "\n",
        "all_names = dict()\n",
        "EPOCH = 0\n",
        "id_num = 1\n",
        "\n",
        "class my_Dataset(Dataset):\n",
        "  def __init__(self, root_dir, transform=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      root_dir (string): Directory with all the images\n",
        "      transform (callable, optional): transform to be applied to each image sample\n",
        "    \"\"\"\n",
        "    # Read names of images in the root directory\n",
        "    image_names = os.listdir(root_dir)\n",
        "    global id_num\n",
        "    if (all_names.get(root_dir) == None):\n",
        "        all_names[root_dir] = id_num\n",
        "        id_num += 1  \n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform \n",
        "    self.image_names = natsorted(image_names)\n",
        "    self.tag = all_names[root_dir]\n",
        "\n",
        "  def __len__(self): \n",
        "    return len(self.image_names)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Get the path to the image \n",
        "    img_path = os.path.join(self.root_dir, self.image_names[idx])\n",
        "    # Load image and convert it to RGB\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    # Apply transformations to the image\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    return img, self.tag\n",
        "\n",
        "\n",
        "class VAEXperiment(pl.LightningModule):\n",
        "\n",
        "    def __init__(self,\n",
        "                 vae_model: BaseVAE,\n",
        "                 logger: TestTubeLogger,\n",
        "                 params: dict) -> None:\n",
        "        super(VAEXperiment, self).__init__()\n",
        "\n",
        "        self.model = vae_model\n",
        "        self.params = params\n",
        "        self.logger = tt_logger\n",
        "        self.curr_device = None\n",
        "        self.hold_graph = False\n",
        "        try:\n",
        "            self.hold_graph = self.params['retain_first_backpass']\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def forward(self, input: Tensor, **kwargs) -> Tensor:\n",
        "        return self.model(input, **kwargs)\n",
        "\n",
        "    def training_step(self, batch, batch_idx, target, optimizer_idx = 0):\n",
        "        real_img = batch\n",
        "        self.curr_device = real_img.device\n",
        "        results = self.forward(real_img)\n",
        "        train_loss = self.model.loss_function(*results,\n",
        "                                              M_N = self.params['batch_size']/ self.num_train_imgs,\n",
        "                                              optimizer_idx=optimizer_idx,\n",
        "                                              batch_idx=batch_idx,\n",
        "                                              target=target)\n",
        "        self.logger.experiment.log({key: val.item() for key, val in train_loss.items()})\n",
        "        return train_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx, optimizer_idx = 0):\n",
        "        real_img = batch\n",
        "        self.curr_device = real_img.device\n",
        "\n",
        "        results = self.forward(real_img)\n",
        "        val_loss = self.model.loss_function(*results,\n",
        "                                            M_N = self.params['batch_size']/ self.num_val_imgs,\n",
        "                                            optimizer_idx = optimizer_idx,\n",
        "                                            batch_idx = batch_idx)\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "    def validation_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "        tensorboard_logs = {'avg_val_loss': avg_loss}\n",
        "        self.sample_images()\n",
        "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def sample_images(self):\n",
        "        # Get sample reconstruction image\n",
        "        test_input = next(iter(self.sample_dataloader))[0]\n",
        "        test_input = test_input.to(self.curr_device)\n",
        "        recons = self.model.generate(test_input)\n",
        "        vutils.save_image(torch.cat((test_input,recons.data)),\n",
        "                          f\"{self.logger.save_dir}{self.logger.name}/version_{self.logger.version}/\"\n",
        "                          f\"recons_{self.logger.name}_{EPOCH}.png\",\n",
        "                          normalize=True,\n",
        "                          nrow=5)\n",
        "\n",
        "        # vutils.save_image(test_input.data,\n",
        "        #                   f\"{self.logger.save_dir}{self.logger.name}/version_{self.logger.version}/\"\n",
        "        #                   f\"real_img_{self.logger.name}_{self.current_epoch}.png\",\n",
        "        #                   normalize=True,\n",
        "        #                   nrow=12)\n",
        "\n",
        "        try:\n",
        "            samples = self.model.sample(25,\n",
        "                                        self.curr_device,\n",
        "                                        labels = test_label)\n",
        "            vutils.save_image(samples.cpu().data,\n",
        "                              f\"{self.logger.save_dir}{self.logger.name}/version_{self.logger.version}/\"\n",
        "                              f\"{self.logger.name}_{EPOCH}.png\",\n",
        "                              normalize=True,\n",
        "                              nrow=12)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "        del test_input, recons #, samples\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "\n",
        "        optims = []\n",
        "        scheds = []\n",
        "\n",
        "        optimizer = optim.Adam(self.model.parameters(),\n",
        "                               lr=self.params['LR'],\n",
        "                               weight_decay=self.params['weight_decay'])\n",
        "        optims.append(optimizer)\n",
        "        # Check if more than 1 optimizer is required (Used for adversarial training)\n",
        "        try:\n",
        "            if self.params['LR_2'] is not None:\n",
        "                optimizer2 = optim.Adam(getattr(self.model,self.params['submodel']).parameters(),\n",
        "                                        lr=self.params['LR_2'])\n",
        "                optims.append(optimizer2)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            if self.params['scheduler_gamma'] is not None:\n",
        "                scheduler = optim.lr_scheduler.ExponentialLR(optims[0],\n",
        "                                                             gamma = self.params['scheduler_gamma'])\n",
        "                scheds.append(scheduler)\n",
        "\n",
        "                # Check if another scheduler is required for the second optimizer\n",
        "                try:\n",
        "                    if self.params['scheduler_gamma_2'] is not None:\n",
        "                        scheduler2 = optim.lr_scheduler.ExponentialLR(optims[1],\n",
        "                                                                      gamma = self.params['scheduler_gamma_2'])\n",
        "                        scheds.append(scheduler2)\n",
        "                except:\n",
        "                    pass\n",
        "                return optims, scheds\n",
        "        except:\n",
        "            return optims\n",
        "\n",
        "    @data_loader\n",
        "    def train_dataloader(self):\n",
        "        transform = self.data_transforms()\n",
        "        all_datasets = []\n",
        "        for i in qwerty:\n",
        "            all_datasets.append(my_Dataset(\"/content/gdrive/MyDrive/\" + i, transform))\n",
        "        \n",
        "        dataset = ConcatDataset(all_datasets)\n",
        "\n",
        "        self.num_train_imgs = len(dataset)\n",
        "        return DataLoader(dataset,\n",
        "                          batch_size= self.params['batch_size'],\n",
        "                          shuffle = True,\n",
        "                          drop_last=True)\n",
        "\n",
        "\n",
        "    @data_loader\n",
        "    def val_dataloader(self):\n",
        "        transform = self.data_transforms()\n",
        "        all_datasets = []\n",
        "        for i in qwerty:\n",
        "            all_datasets.append(my_Dataset(\"/content/gdrive/MyDrive/\" + i, transform))\n",
        "        \n",
        "        dataset = ConcatDataset(all_datasets)\n",
        "        self.sample_dataloader =  DataLoader(dataset,\n",
        "                                            batch_size= 25,\n",
        "                                            shuffle = True,\n",
        "                                            drop_last=True)\n",
        "\n",
        "        self.num_val_imgs = len(self.sample_dataloader)\n",
        "\n",
        "        return self.sample_dataloader\n",
        "\n",
        "    def data_transforms(self):\n",
        "\n",
        "        SetRange = transforms.Lambda(lambda X: 2 * X - 1.)\n",
        "        SetScale = transforms.Lambda(lambda X: X/X.sum(0).expand_as(X))\n",
        "\n",
        "        if self.params['dataset'] == 'celeba':\n",
        "            transform = transforms.Compose([\n",
        "                                            transforms.ToTensor()])\n",
        "        else:\n",
        "            raise ValueError('Undefined dataset type')\n",
        "        return transform\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdkZRVbWC2pP"
      },
      "source": [
        "Основной код для запуска"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qQSVz128sRc"
      },
      "source": [
        "model = LogCoshVAE(\n",
        "  in_channels= 3,\n",
        "  latent_dim= 128,\n",
        "  alpha= 10.0,\n",
        "  beta= 1.0,\n",
        "  gamma1= 0.01, #близость из одного класса для матожидания\n",
        "  teta1= 0.01, #близость из одного класса для дисперсии\n",
        "  gamma2= 0.01, #дальность из разных классов\n",
        "  teta2= 0.01)#.to(\"cuda\")                                                                                 !!!\n",
        "\n",
        "tt_logger = TestTubeLogger(\n",
        "    save_dir=\"logs/\",\n",
        "    name=\"LogCoshVAE\",\n",
        "    debug=False,\n",
        "    create_git_tag=False,\n",
        ")\n",
        "\n",
        "exp_params = {\n",
        "  'dataset': 'celeba',\n",
        "  'data_path': \"../../shared/momo/Data/\",\n",
        "  'img_size': 64,\n",
        "  'batch_size': 64, # Better to have a square number\n",
        "  'LR': 0.005,\n",
        "  'weight_decay': 0.0,\n",
        "  'scheduler_gamma': 0.97\n",
        "}\n",
        "\n",
        "checkpoint = torch.load(\"/content/gdrive/MyDrive/checkpoints/29model3.pt\", map_location=torch.device('cpu')) #      !!!! Поменять \"cpu\" на \"cuda\" если обучаешь на видеокарте\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "exper = VAEXperiment(model,tt_logger, exp_params)#.to(\"cuda\")\n",
        "\n",
        "\n",
        "optimiz = exper.configure_optimizers() [0][0]\n",
        "optimiz.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "qwerty = [\"new_alisa\", \"new_ilnur\", \"new_ilyas\", \"new_misha\", \"new_vanya\"]  # место хранения фотографий разных людей\n",
        "dataset = exper.train_dataloader()\n",
        "val = exper.val_dataloader()\n",
        "for i in range(20):\n",
        "    loss = 0\n",
        "    EPOCH = i\n",
        "    for batch_idx, (data, target) in enumerate(dataset):\n",
        "        optimiz.zero_grad()\n",
        "        loss = exper.training_step(data, batch_idx, target) #                       !!!!! .to(\"cuda\") если на видеокарте\n",
        "        loss['loss'].backward()\n",
        "        optimiz.step()\n",
        "    # exper.sample_images()                    # в PyTorch-VAE/logs будут показывать примеры картинок\n",
        "    torch.save({\n",
        "            'epoch': i,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimiz.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, \"/content/gdrive/MyDrive/checkpoints/\" + str(i) + \"model3.pt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKMI0k5f4fPb"
      },
      "source": [
        "Протестировать модель на фотографии"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "I4Hq40LndYU3",
        "outputId": "444cac5b-068d-4d06-8066-259612168344"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import cv2\n",
        "photo = \"/content/gdrive/MyDrive/new_ilnur/ilnur240.jpg\"\n",
        "d = torch.from_numpy(np.array([cv2.imread(photo).transpose(2,0,1)]))\n",
        "a = model.generate(d.type(torch.FloatTensor))#       !!!! добавить cuda если на видеокарте\n",
        "b = a.to(\"cpu\").detach().numpy()[0].transpose(1,2,0)\n",
        "plt.imshow(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcff0009d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVPIdn5o4_IM"
      },
      "source": [
        "Провести анализ первых компонент:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTbJPwyYd4PR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "photo1 = \"/content/gdrive/MyDrive/new_alisa/alisa240.jpg\"\n",
        "photo2 = \"/content/gdrive/MyDrive/new_alisa/alisa2405.jpg\"\n",
        "photo3 = \"/content/gdrive/MyDrive/new_ilnur/ilnur2405.jpg\"\n",
        "d = torch.from_numpy(np.array([cv2.imread(photo1).transpose(2,0,1)]))\n",
        "c = torch.from_numpy(np.array([cv2.imread(photo2).transpose(2,0,1)]))\n",
        "e = torch.from_numpy(np.array([cv2.imread(photo3).transpose(2,0,1)]))\n",
        "a, b = model.encode(d.type(torch.FloatTensor))\n",
        "a2, b2 = model.encode(c.type(torch.FloatTensor))\n",
        "a3, b3 = model.encode(e.type(torch.FloatTensor))\n",
        "ch = []\n",
        "for i in range(len(a[0])):\n",
        "    print( \"%.5f\" % abs(a[0][i]- a2[0][i]),  \"%.5f\" % abs(a2[0][i] - a3[0][i]))\n",
        "    ch.append(int(abs(a[0][i]- a2[0][i]) < abs(a2[0][i] - a3[0][i])))\n",
        "print(\"Доля близких компонент среди первых = \", (np.array(ch[:48]) == 1).sum() / len(ch[:48]))\n",
        "print(\"Доля близких компонент среди последних = \", (np.array(ch[49:]) == 1).sum() / len(ch[49:]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}